{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle & Analyze WeRateDogs Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to put into practice Data\n",
    "wrangling technics I’ve learned in the Udacity Data Analysis Nanodegree\n",
    "program. <br>\n",
    "The dataset that I will be wrangling is the tweet archive of Twitter user\n",
    "@dog_rates , also known as  WeRateDogs. WeRateDogs is a Twitter account\n",
    "that rates people's dogs with a humorous comment about the dog. These\n",
    "ratings almost always have a denominator of 10. The numerators, though?\n",
    "Almost always greater than 10. (11/10, 12/10, 13/10)​."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project details\n",
    "\n",
    "The goal of this project is to apply Data Wrangling technics on a dataset and retrieve insights. \n",
    "\n",
    "The Data Wrangling processes are as follows:\n",
    "\n",
    "        - Gathering data\n",
    "        - Assessing data\n",
    "        - Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "In this part, we have gathered data from multiple sources.\n",
    "<br><br>\n",
    "\n",
    "    1- the Twitter archive: \n",
    "    \n",
    "    This archive contains basic tweet data for\n",
    "    all 5000+ of their tweets as they stood on August 1, 2017. \n",
    "    The twitter-archive-enhanced.csv was provided by Udacity, downloaded\n",
    "    manually then was loaded from the CSV file into a pandas data frame\n",
    "\n",
    "    2- The tweet image predictions:\n",
    "    \n",
    "    This file contains the top three predictions of dog breed for each dog image\n",
    "    from the WeRateDogs Enhanced Twitter Archive. Data is downloaded\n",
    "    programmatically using the Requests library from the URL address into a\n",
    "    tsv file. The content of image-predictions.tsv file is then loaded into the\n",
    "    pandas' data frame, with the size of 2075 rows and 11 columns.\n",
    "    The table contains the top three predictions, tweet ID, image URL, and the\n",
    "    image number that corresponded to the most confident prediction.\n",
    "    \n",
    "    3-  Twitter API File:\n",
    "    \n",
    "    Twitter API file contains tweet id, favorite count and retweet count. Data\n",
    "    was provided by Udacity, downloaded manually then was loaded from the\n",
    "    tweet-json.txt file into a pandas data frame. Dataframe size is 2354 rows\n",
    "    and 2 columns. The tweeter ID column is used as an index.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "</h4>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part We assessed both visually and programmatically these datasets to find some issues they could have.\n",
    "\n",
    "There is what we found:\n",
    "\n",
    "### Quality issues\n",
    "\n",
    "##### Twitter_archive\n",
    "\n",
    "1. the source column contains html code\n",
    "\n",
    "2. the datatype of the timestamp column is object and should be datetime\n",
    "\n",
    "3. some of the dog names are not correct (None, a, an, the)\n",
    "\n",
    "4. some of the ratings are not correctly extracted \n",
    "\n",
    "5. some of the dogs are not classified as one of \"doggo\", \"floofer\", \"pupper\" or \"puppo\" and contain all \"None\" instead\n",
    "\n",
    "\n",
    "##### Image_prediction\n",
    "\n",
    "6. there is a \"_\" instead of a whitespace in the predictions\n",
    "\n",
    "7. the predictions are sometimes uppercase, sometimes lowercase in the image_prediction dataset\n",
    "\n",
    "8. there are pictures in the image_prediction dataset that are not dogs \n",
    "\n",
    "### Tidiness issues\n",
    "1. the columns doggo, floofer,pupper and puppo should be combined in one column \n",
    "\n",
    "2. the three tables share the \"tweet_id\" column so they should be merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we solve the issues we have found earlier and some others to make data ready <br>\n",
    "to get insights and Data driven decisions.\n",
    "\n",
    "After the Cleaning phase we stored the new dataset (resulting from merging the three originals ones) <br>\n",
    "into a csv file named twitter_archive_master.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
